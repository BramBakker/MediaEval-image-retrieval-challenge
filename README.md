**MediaEval2025_NewsImage_Das-RU_RET**

This project is developed for the MediaEval 2025 NewsImages challenge, focusing on the image retrieval subtask.

(Reference: [MediaEval 2025 NewsImages Task Link](https://multimediaeval.github.io/editions/2025/tasks/newsimages/))

This method uses the Long-CLIP model to make use of longer pieces of text, including the first sentences of the articles to create a better fit. 
https://github.com/beichenzbc/Long-CLIP. 

To get the article text I used the newspaper3k python package.https://pypi.org/project/newspaper3k/

The specific method of incorporating categories in the embedings of the news articles I used comes from a paper from Song, Rui, et al. "Improving News Retrieval with a Learnable Alignment Module for Multimodal Text–Image Matching." Electronics 14.15 (2025): 3098.  

https://www.mdpi.com/2079-9292/14/15/3098 

This method consists of an extra label predictor to learn the relationship between text and news categories from datasets like the n24 dataset (https://www.kaggle.com/datasets/ritabrata123/n24news-zip
), 60.000 new york times articles with images and 24 corresponding categories like 'economy' or 'science'. These are then used to project the embeddings. 

This was done because of the multitude of ways images align with text, perhaps alignment could be generalized across newspaper categories to get more consistent retrieval results. The method use three different loss components, the contrastive loss between text and corresponding image embedding, the label loss, and then the KL divergence loss, which makes sure the embeddings generated by the model aren’t too far from the original embeddings generated by CLIP. 

After encoding the yfcc100m dataset with the longclip model, FAISS is used to compare three types of embeddings (https://github.com/facebookresearch/faiss). one of the fine-tuned label aligned model, another with the standard longclip model that just uses headlines, and one that uses headlines + text.

