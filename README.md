*Image retrieval techniques*

This method uses the Long-CLIP model to make use of longer pieces of text, including the first sentences of the articles to create a better fit. 
https://github.com/beichenzbc/Long-CLIP. 

To get the article text I used the newspaper3k python package.https://pypi.org/project/newspaper3k/

The specific method of incorporating categories in the embedings of the news articles I used comes from a paper from Song, Rui, et al. "Improving News Retrieval with a Learnable Alignment Module for Multimodal Text–Image Matching." Electronics 14.15 (2025): 3098.  

https://www.mdpi.com/2079-9292/14/15/3098 

This method consists of an extra label predictor, which learns to match labels based on classes from datasets like the nyt24 dataset, 60.000 new york times articles with images and 24 corresponding categories like 'economy' or 'science'. 
https://www.kaggle.com/datasets/ritabrata123/n24news-zip

This was done because of the multitude of ways images align with text, and those could perhaps be generalized across newspaper categories to get more consistent retrieval results. The method use three different loss components, the contrastive loss between text and corresponding image embedding, the label loss, which further projects the embedding, and then the KL divergence loss, which makes sure the embeddings generated by the model aren’t too far from the original embeddings generated by CLIP. 

After encoding the yfcc100m dataset with the lonclip model, FAISS is used to compare the embeddings of the fine-tuned longclip model, a model that just uses headlines, and one that just uses text to see which images they retrieve.
https://github.com/facebookresearch/faiss
 
